{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../src/reload.py\n",
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parsed logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                | 1/19468 [00:00<45:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making tokens db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19468/19468 [00:30<00:00, 637.19it/s]\n",
      "  1%|█▎                                                                                                                                          | 197/21933 [00:00<00:15, 1437.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making swaps db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21933/21933 [00:11<00:00, 1970.43it/s]\n",
      "  3%|████▌                                                                                                                                       | 633/19468 [00:00<00:06, 3139.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating tokens with info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19468/19468 [00:06<00:00, 2821.29it/s]\n",
      "  7%|█████████▎                                                                                                                                 | 1309/19468 [00:00<00:02, 6308.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding artifact file size data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19468/19468 [00:02<00:00, 7220.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing result data...\n",
      "written 13502287 bytes 19468 entries\n",
      "written 2232387 bytes 21933 entries\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "print('Reading parsed logs...')\n",
    "\n",
    "nft_state_log = src.utils.read_json(src.config.nft_state_log_file)\n",
    "nft_state = src.contracts.nft_state.NFTState()\n",
    "nft_state_replayer = src.contracts.state_utils.StateReplayer(nft_state_log, nft_state)\n",
    "\n",
    "ah_state_log = src.utils.read_json(src.config.ah_state_log_file)\n",
    "ah_state = src.contracts.art_house_state.ArtHouseState()\n",
    "ah_state_replayer = src.contracts.state_utils.StateReplayer(ah_state_log, ah_state)\n",
    "\n",
    "money_state_log = src.utils.read_json(src.config.money_state_log_file)\n",
    "money_state = src.contracts.money_state.MoneyState()\n",
    "money_state_replayer = src.contracts.state_utils.StateReplayer(money_state_log, money_state)\n",
    "\n",
    "addrs_state_log = src.utils.read_json(src.config.addrs_state_log_file)\n",
    "addrs_state = src.contracts.addrs_state.AddrsState()\n",
    "addrs_state_replayer = src.contracts.state_utils.StateReplayer(addrs_state_log, addrs_state)\n",
    "\n",
    "tr_info_db = src.tr.info_db.TrInfoDB()\n",
    "\n",
    "nft_state_replayer.replay_to_end()\n",
    "ah_state_replayer.replay_to_end()\n",
    "money_state_replayer.replay_to_end()\n",
    "addrs_state_replayer.replay_to_end()\n",
    "\n",
    "\n",
    "print('Making tokens db...')\n",
    "\n",
    "tokens_db = {}\n",
    "swaps_db = {}\n",
    "\n",
    "assert set(nft_state.tokens.keys()) == set(ah_state.tokens.keys())\n",
    "\n",
    "for token_id, nft_token_info in nft_state.tokens.items():\n",
    "    tokens_db[str(token_id)] = {\n",
    "        'token_id': token_id,\n",
    "        'creator': nft_token_info['creator'],\n",
    "        'mint_tokens_receiver': nft_token_info['tokens_receiver'],\n",
    "        'info_ipfs': nft_token_info['info_ipfs'],\n",
    "        'mint_count': nft_token_info['mint_count'],\n",
    "        'mint_row_id': nft_token_info['mint_row_id'],\n",
    "    }\n",
    "\n",
    "for token_id, ah_token_info in tqdm(ah_state.tokens.items()):\n",
    "    db_entry = tokens_db[str(token_id)]\n",
    "    db_entry['royalties'] = ah_token_info['royalties']\n",
    "    db_entry['mint_ah_row_id'] = ah_token_info['mint_ah_row_id']\n",
    "\n",
    "    assert db_entry['mint_tokens_receiver'] == ah_token_info['tokens_receiver']\n",
    "    assert db_entry['mint_count'] == ah_token_info['mint_count']\n",
    "    assert db_entry['info_ipfs'] == ah_token_info['info_ipfs']\n",
    "\n",
    "    nft_mint_tr = tr_info_db.get_full_tr_by_row_id(db_entry['mint_row_id'])\n",
    "    ah_mint_tr = tr_info_db.get_full_tr_by_row_id(ah_token_info['mint_ah_row_id'])\n",
    "    assert nft_mint_tr == ah_mint_tr\n",
    "\n",
    "\n",
    "print('Making swaps db...')\n",
    "\n",
    "for swap_id, ah_swap_info in tqdm(list(ah_state.swaps.items())):\n",
    "    assert str(ah_swap_info['token_id']) in tokens_db\n",
    "    ah_swap_op = tr_info_db.get_full_tr_by_row_id(ah_swap_info['created_row_id'], return_op=True)\n",
    "    swaps_db[str(swap_id)] = {\n",
    "        'swap_id': swap_id,\n",
    "        'created_row_id': ah_swap_info['created_row_id'],\n",
    "        'token_id': ah_swap_info['token_id'],\n",
    "        'price': ah_swap_info['price'],\n",
    "        'initial_count': ah_swap_info['initial_count'],\n",
    "    }\n",
    "\n",
    "\n",
    "print('Populating tokens with info...')\n",
    "\n",
    "token_info_fields_cnt = Counter()\n",
    "\n",
    "for token_id, token_db_entry in tqdm(tokens_db.items()):\n",
    "    info_ipfs_fpath = src.ipfs.get_ipfs_fpath(token_db_entry['info_ipfs'], 'ipfs0')\n",
    "    token_info = src.utils.read_json(info_ipfs_fpath)\n",
    "\n",
    "    for field in token_info:\n",
    "        token_info_fields_cnt[field] += 1\n",
    "    token_info_keys = set(token_info.keys())\n",
    "    assert token_info_keys.issubset({\n",
    "        'artifactUri', 'creators', 'decimals', 'description', 'displayUri', 'formats',\n",
    "        'isBooleanAmount', 'name', 'shouldPreferSymbol', 'symbol', 'tags', 'thumbnailUri',\n",
    "    })\n",
    "\n",
    "    if int(token_id) == 152:\n",
    "        assert 'isBooleanAmount' not in token_info\n",
    "    elif int(token_id) <= 352:\n",
    "        assert token_info['isBooleanAmount'] == (token_db_entry['mint_count'] == 1)\n",
    "    else:\n",
    "        assert token_info['isBooleanAmount'] is False\n",
    "\n",
    "    if int(token_id) <= 154:\n",
    "        assert 'shouldPreferSymbol' not in token_info\n",
    "    else:\n",
    "        assert token_info['shouldPreferSymbol'] is False\n",
    "\n",
    "    if 'displayUri' in token_info:\n",
    "        assert int(token_id) >= 11000\n",
    "        if token_info['displayUri']:\n",
    "            assert src.ipfs.validate_ipfs_uri(token_info['displayUri'])\n",
    "\n",
    "    assert token_info['decimals'] == 0\n",
    "    assert token_info['thumbnailUri'] == 'ipfs://QmNrhZHUaEqxhyLfqoq1mtHSipkWHeT31LNHb1QEbDHgnc'\n",
    "    assert token_info['symbol'] == 'OBJKT'\n",
    "\n",
    "    assert len(token_info['formats']) == 1\n",
    "    assert token_info['formats'][0]['uri'] == token_info['artifactUri']\n",
    "    assert src.ipfs.validate_ipfs_uri(token_info['formats'][0]['uri'])\n",
    "    assert len(token_info['creators']) == 1\n",
    "\n",
    "    for tag in token_info['tags']:\n",
    "        for ch in [' ', '\\n', '\\t']:\n",
    "            assert ch not in tag, (ch, tag)\n",
    "\n",
    "    if token_info['creators'][0] != token_db_entry['creator']:\n",
    "        assert token_id == '5571' or token_info['creators'][0] in ['', None]\n",
    "\n",
    "    token_db_entry['artifact_mime'] = token_info['formats'][0]['mimeType']\n",
    "    token_db_entry['artifact_ipfs'] = token_info['formats'][0]['uri']\n",
    "    token_db_entry['meta_creator'] = str(token_info['creators'][0])\n",
    "    token_db_entry['display_uri_ipfs'] = token_info.get('displayUri', '') or ''\n",
    "    token_db_entry['tags'] = '\\n'.join(token_info['tags'])\n",
    "    token_db_entry['name'] = token_info['name']\n",
    "    token_db_entry['description'] = token_info['description']\n",
    "\n",
    "\n",
    "print('Adding artifact file size data...')\n",
    "\n",
    "not_in_cache_count = 0\n",
    "for token_id, token_db_entry in tqdm(tokens_db.items()):\n",
    "    try:\n",
    "        artifact_fpath = src.ipfs.get_ipfs_fpath(token_db_entry['artifact_ipfs'], 'ipfs1')\n",
    "        token_db_entry['artifact_file_size'] = artifact_fpath.stat().st_size\n",
    "    except IpfsNotCachedException:\n",
    "        not_in_cache_count += 1\n",
    "\n",
    "    if token_db_entry['display_uri_ipfs'] != '':\n",
    "        try:\n",
    "            display_uri_fpath = src.ipfs.get_ipfs_fpath(token_db_entry['display_uri_ipfs'], 'ipfs1')\n",
    "            token_db_entry['display_uri_file_size'] = display_uri_fpath.stat().st_size\n",
    "        except IpfsNotCachedException:\n",
    "            not_in_cache_count += 1\n",
    "    else:\n",
    "        token_db_entry['display_uri_file_size'] = -1\n",
    "\n",
    "print('Number of tokens without downloaded artifacts:', not_in_cache_count)\n",
    "\n",
    "print('Writing result data...')\n",
    "\n",
    "src.utils.write_json(tokens_db, src.config.tokens_db_json_file)\n",
    "src.utils.write_json(swaps_db, src.config.swaps_db_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown prefixes: {'tz-wrapped/'}\n",
      "Writing addresses database...\n",
      "written 994526 bytes 4105 entries\n"
     ]
    }
   ],
   "source": [
    "addrs_db = {}\n",
    "for addr, addr_entry in addrs_state.addrs.items():\n",
    "    addrs_db[addr] = {\n",
    "        'address': addr,\n",
    "        'first_op_row_id': addr_entry['first_op_row_id'],\n",
    "        'first_op_has_reveal': addr_entry['first_op_has_reveal'],\n",
    "        'in_op_count': addr_entry['in_op_count'],\n",
    "        'out_op_count': addr_entry['out_op_count'],\n",
    "        'money_received': addr_entry['money_received'],\n",
    "        'money_sent': addr_entry['money_sent'],\n",
    "    }\n",
    "\n",
    "accounts_metadata = src.utils.read_json(src.config.tzktio_accounts_metadata_file)\n",
    "\n",
    "url_prefixes = {\n",
    "    'https://www.facebook.com/',\n",
    "    'www.facebook.com/',\n",
    "    'https://facebook.com/',\n",
    "    'https://www.instagram.com/',\n",
    "    'https://instagram.com/',\n",
    "    'https://web.facebook.com/',\n",
    "}\n",
    "\n",
    "unknown_prefixes = set()\n",
    "for addr, addr_meta in accounts_metadata.items():\n",
    "    if addr_meta is None:\n",
    "        continue\n",
    "    db_entry = addrs_db[addr]\n",
    "    for field, field_val in addr_meta.items():\n",
    "        if field in {'twitter', 'site', 'email', 'instagram', 'github', 'telegram', 'reddit', 'facebook'}:\n",
    "            orig_field_val = field_val\n",
    "            field_val = field_val.strip()\n",
    "            if field_val.endswith('/'):\n",
    "                field_val = field_val[:-1]\n",
    "            if field not in {'site', 'email', 'reddit'} and '/' in field_val:\n",
    "                prefix = '/'.join(field_val.lower().split('/')[:-1]) + '/'\n",
    "                if prefix in url_prefixes:\n",
    "                    field_val = field_val[len(prefix):]\n",
    "                else:\n",
    "                    unknown_prefixes.add(prefix)\n",
    "            db_entry[f'tzkt_info_{field}'] = field_val\n",
    "        elif field == 'alias':\n",
    "            db_entry['tzkt_info_name'] = field_val\n",
    "\n",
    "if unknown_prefixes:\n",
    "    print(f'Unknown prefixes: {unknown_prefixes}')\n",
    "\n",
    "print('Writing addresses database...')\n",
    "src.utils.write_json(addrs_db, src.config.addrs_db_json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
