{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is boilerplate code for all jupyter notebooks which loads all python modules in `/src` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path('..').resolve()\n",
    "assert repo_dir.name == 'hicetnunc-dataset', repo_dir\n",
    "if str(repo_dir) not in sys.path:\n",
    "    sys.path.append(str(repo_dir))\n",
    "\n",
    "import src.reload; src.reload.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_db = src.utils.read_json(src.config.tokens_db_json_file)\n",
    "swaps_db = src.utils.read_json(src.config.swaps_db_json_file)\n",
    "addrs_db = src.utils.read_json(src.config.addrs_db_json_file)\n",
    "\n",
    "nft_state_log = src.utils.read_json(src.config.nft_state_log_file)\n",
    "ah_state_log = src.utils.read_json(src.config.ah_state_log_file)\n",
    "money_state_log = src.utils.read_json(src.config.money_state_log_file)\n",
    "\n",
    "tr_info_db = src.tr.info_db.TrInfoDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join contracts logs with transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "transactions_index = defaultdict(lambda: {\n",
    "    'nft_log_entry': None,\n",
    "    'ah_log_entry': None,\n",
    "    'money_log_entry': None,\n",
    "    'created_swap': None,\n",
    "    'created_token': None,\n",
    "})\n",
    "\n",
    "\n",
    "for entries, row_id_field, entry_field in [\n",
    "    (nft_state_log, 'row_id', 'nft_log_entry'),\n",
    "    (ah_state_log, 'row_id', 'ah_log_entry'),\n",
    "    (money_state_log, 'row_id', 'money_log_entry'),\n",
    "    (swaps_db.values(), 'created_row_id', 'created_swap'),\n",
    "    (tokens_db.values(), 'mint_ah_row_id', 'created_token'),\n",
    "]:\n",
    "    for entry in entries:\n",
    "        tr_hash = tr_info_db.get_tr_info_by_row_id(entry[row_id_field])['hashc']\n",
    "        assert transactions_index[tr_hash][entry_field] is None\n",
    "        transactions_index[tr_hash][entry_field] = entry\n",
    "        entry['tr_index_entry'] = transactions_index[tr_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check, that every art house log entry has corresponding nft log entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ah_log_entry in ah_state_log:\n",
    "    entry_method = ah_log_entry['method']\n",
    "    nft_log_entry = ah_log_entry['tr_index_entry']['nft_log_entry']\n",
    "    if entry_method in ['apply_collect', 'apply_swap', 'apply_cancel_swap']:\n",
    "        assert nft_log_entry['method'] == 'apply_transfer'\n",
    "    elif entry_method == 'apply_mint':\n",
    "        assert nft_log_entry['method'] == 'apply_mint'\n",
    "    else:\n",
    "        assert False, entry_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldsGroup:\n",
    "    def expand_fields(self, prefix):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class TrEvent(FieldsGroup):\n",
    "    def __init__(self, row_id=-1):\n",
    "        self.row_id = row_id\n",
    "\n",
    "    def set_row_id(self, row_id):\n",
    "        self.row_id = row_id\n",
    "\n",
    "    def expand_fields(self, prefix):\n",
    "        info = tr_info_db.get_tr_info_by_row_id(self.row_id)\n",
    "        return {\n",
    "            f'{prefix}_iso_date': info['iso_date'],\n",
    "            f'{prefix}_stamp': info['stamp'],\n",
    "            f'{prefix}_hash': info['hash'],\n",
    "            f'{prefix}_row_id': info['row_id'],\n",
    "        }\n",
    "\n",
    "\n",
    "class FloatSet(FieldsGroup):\n",
    "    def __init__(self, values=None):\n",
    "        self.zero_count = 0\n",
    "        self.values = values or []\n",
    "\n",
    "    def add(self, v):\n",
    "        if v == 0:\n",
    "            self.zero_count += 1\n",
    "        else:\n",
    "            self.values.append(v)\n",
    "\n",
    "    def expand_fields(self, prefix):\n",
    "        prefix1 = '_'.join(prefix.split('_')[:-1])\n",
    "        prefix2 = prefix\n",
    "        if len(self.values) == 0:\n",
    "            return {\n",
    "                f'{prefix1}_count': 0,\n",
    "                f'{prefix1}_zero_count': self.zero_count,\n",
    "                f'{prefix2}_min': 0,\n",
    "                f'{prefix2}_max': 0,\n",
    "                f'{prefix2}_sum': 0,\n",
    "                f'{prefix2}_avg': 0,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                f'{prefix1}_count': len(self.values),\n",
    "                f'{prefix1}_zero_count': self.zero_count,\n",
    "                f'{prefix2}_min': min(self.values),\n",
    "                f'{prefix2}_max': max(self.values),\n",
    "                f'{prefix2}_sum': sum(self.values),\n",
    "                f'{prefix2}_avg': sum(self.values) / len(self.values),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ds = {}\n",
    "for token_db_entry in tokens_db.values():\n",
    "    tokens_ds[str(token_db_entry['token_id'])] = {\n",
    "        'token_id': str(token_db_entry['token_id']),\n",
    "        'creator': token_db_entry['creator'],\n",
    "        'mint_count': token_db_entry['mint_count'],\n",
    "        'mint': TrEvent(token_db_entry['mint_row_id']),\n",
    "        'artifact_ipfs': src.ipfs.validate_ipfs_uri(token_db_entry['artifact_ipfs']),\n",
    "        'artifact_mime': token_db_entry['artifact_mime'],\n",
    "        'artifact_file_size': token_db_entry['artifact_file_size'],\n",
    "        'info_title': token_db_entry['name'],\n",
    "        'info_description': token_db_entry['description'],\n",
    "        'info_tags': token_db_entry['tags'],\n",
    "        'author_sold_prices': FloatSet(),\n",
    "        'secondary_sold_prices': FloatSet(),\n",
    "        'author_sold_prices': FloatSet(),\n",
    "        'available_prices': FloatSet(),\n",
    "        'burn_count': 0,\n",
    "        'author_owns_count': 0,\n",
    "        'other_own_count': 0,\n",
    "        'author_sent_count': 0,\n",
    "        'info_ipfs': src.ipfs.validate_ipfs_uri(token_db_entry['info_ipfs']),\n",
    "        'display_uri_ipfs': (\n",
    "            src.ipfs.validate_ipfs_uri(token_db_entry['display_uri_ipfs'])\n",
    "            if token_db_entry['display_uri_ipfs'] else ''\n",
    "        ),\n",
    "        'royalties': token_db_entry['royalties'] / 10,\n",
    "        'info_creator': token_db_entry['meta_creator'],\n",
    "        'mint_ah_row_id': token_db_entry['mint_ah_row_id'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "addrs_ds = {}\n",
    "for addr_db_entry in addrs_db.values():\n",
    "    addrs_ds[addr_db_entry['address']] = {\n",
    "        'address': addr_db_entry['address'],\n",
    "        'first_action': TrEvent(addr_db_entry['first_op_row_id']),\n",
    "        'tzkt_info_name': addr_db_entry.get('tzkt_info_name', ''),\n",
    "        'tzkt_info_twitter': addr_db_entry.get('tzkt_info_twitter', ''),\n",
    "        'tzkt_info_email': addr_db_entry.get('tzkt_info_email', ''),\n",
    "        'tzkt_info_instagram': addr_db_entry.get('tzkt_info_instagram', ''),\n",
    "        'tzkt_info_site': addr_db_entry.get('tzkt_info_site', ''),\n",
    "        'tzkt_info_description': addr_db_entry.get('tzkt_info_description', ''),\n",
    "        'tzkt_info_github': addr_db_entry.get('tzkt_info_github', ''),\n",
    "        'tzkt_info_telegram': addr_db_entry.get('tzkt_info_telegram', ''),\n",
    "        'tzkt_info_facebook': addr_db_entry.get('tzkt_info_facebook', ''),\n",
    "        'tzkt_info_reddit': addr_db_entry.get('tzkt_info_reddit', ''),\n",
    "        'bought_prices': FloatSet(),\n",
    "        'author_sold_prices': FloatSet(),\n",
    "        'secondary_sold_prices': FloatSet(),\n",
    "        'available_prices': FloatSet(),\n",
    "        'in_op_count': addr_db_entry['in_op_count'],\n",
    "        'out_op_count': addr_db_entry['out_op_count'],\n",
    "        'money_received': addr_db_entry['money_received'],\n",
    "        'money_sent': addr_db_entry['money_sent'],\n",
    "        'first_op_has_reveal': int(addr_db_entry['first_op_has_reveal']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sells_ds = {}\n",
    "transfers_ds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps_ds = {}\n",
    "for swap_db_entry in swaps_db.values():\n",
    "    swaps_ds[str(swap_db_entry['swap_id'])] = {\n",
    "        'swap_id': str(swap_db_entry['swap_id']),\n",
    "        'token_id': str(swap_db_entry['token_id']),\n",
    "        'price': swap_db_entry['price'] / 1e6,\n",
    "        'total_count': swap_db_entry['initial_count'],\n",
    "        'created': TrEvent(swap_db_entry['created_row_id']),\n",
    "        'closed': TrEvent(),\n",
    "        'is_secondary': 1,\n",
    "        'created_by': '',\n",
    "        'sold_count': 0,\n",
    "        'available_count': 0,\n",
    "        'returned_count': 0,\n",
    "        'sold_price_sum': 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_id': 19619,\n",
       " 'creator': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       " 'mint_tokens_receiver': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       " 'info_ipfs': 'ipfs://QmVZRfawGSjroJiH6k1uGa9zNR5VSxEMDSvAvhoBXTb6as',\n",
       " 'mint_count': 3,\n",
       " 'mint_row_id': 44651504,\n",
       " 'royalties': 100,\n",
       " 'mint_ah_row_id': 44651503,\n",
       " 'artifact_mime': 'model/gltf-binary',\n",
       " 'artifact_ipfs': 'ipfs://QmayQGmkYnwrmY4Q72fp3T8wg6fK3YANBMk4RgvmVd44mN',\n",
       " 'meta_creator': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       " 'display_uri_ipfs': '',\n",
       " 'tags': 'Processing\\nCreativecoding\\nGenerativegeometry\\n@wblut',\n",
       " 'name': 'Part 0001',\n",
       " 'description': 'glTF Generative geometry by Frederik Vanhoutte,@wblut',\n",
       " 'artifact_file_size': 1472104,\n",
       " 'display_uri_file_size': -1,\n",
       " 'tr_index_entry': {'nft_log_entry': {'method': 'apply_mint',\n",
       "   'row_id': 44651504,\n",
       "   'token_id': 19619,\n",
       "   'count': 3,\n",
       "   'creator': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       "   'tokens_receiver': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       "   'info_ipfs': 'ipfs://QmVZRfawGSjroJiH6k1uGa9zNR5VSxEMDSvAvhoBXTb6as',\n",
       "   'tr_index_entry': {...}},\n",
       "  'ah_log_entry': {'method': 'apply_mint',\n",
       "   'row_id': 44651503,\n",
       "   'tokens_receiver': 'tz1TSWEDs9wcBx2KiRzVzyzECsNpRiZaLJ1D',\n",
       "   'count': 3,\n",
       "   'info_ipfs': 'ipfs://QmVZRfawGSjroJiH6k1uGa9zNR5VSxEMDSvAvhoBXTb6as',\n",
       "   'royalties': 100,\n",
       "   'tr_index_entry': {...}},\n",
       "  'money_log_entry': None,\n",
       "  'created_swap': None,\n",
       "  'created_token': {...}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_db_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze nft log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'volume'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6e96c8c145a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mentry_method\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'apply_transfer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apply_mint'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mentry_volume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnft_log_entry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'volume'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mah_log_entry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_hash_to_ah_log_entry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnft_log_entry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mmoney_log_entry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_hash_to_money_log_entry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnft_log_entry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'volume'"
     ]
    }
   ],
   "source": [
    "art_house_addr = 'KT1Hkg5qeNhfwpKW4fXvq7HGZB9z2EnmCCA9'\n",
    "trash_addr = 'tz1burnburnburnburnburnburnburjAYjjX'\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "author_stats = {}\n",
    "\n",
    "total_stats = Counter()\n",
    "total_stats['cur_sold_count'] = 0\n",
    "total_stats['cur_sold_volume'] = 0\n",
    "total_stats['cur_author2swap_count'] = 0\n",
    "total_stats['cur_swap2author_count'] = 0\n",
    "total_stats['cur_author2other_count'] = 0\n",
    "total_stats['cur_other2swap_count'] = 0\n",
    "total_stats['cur_swap2other_count'] = 0\n",
    "total_stats['cur_other2author_count'] = 0\n",
    "total_stats['cur_author2trash_count'] = 0\n",
    "total_stats['cur_other2trash_count'] = 0\n",
    "total_stats['cur_other2other_count'] = 0\n",
    "\n",
    "\n",
    "purchases_log = []\n",
    "\n",
    "\n",
    "for token_entry in tokens_db.values():\n",
    "    token_entry['cur_sold_count'] = 0\n",
    "    token_entry['cur_sold_volume'] = 0\n",
    "    token_entry['cur_author2swap_count'] = 0\n",
    "    token_entry['cur_swap2author_count'] = 0\n",
    "    token_entry['cur_author2other_count'] = 0\n",
    "    token_entry['cur_other2swap_count'] = 0\n",
    "    token_entry['cur_swap2other_count'] = 0\n",
    "    token_entry['cur_other2author_count'] = 0\n",
    "    token_entry['cur_author2trash_count'] = 0\n",
    "    token_entry['cur_other2trash_count'] = 0\n",
    "    token_entry['cur_other2other_count'] = 0\n",
    "\n",
    "    token_entry['cur_author_count'] = 0\n",
    "    token_entry['cur_swap_count'] = 0\n",
    "    token_entry['cur_trash_count'] = 0\n",
    "    token_entry['cur_other_count'] = 0\n",
    "\n",
    "for swap_entry in swaps_db.values():\n",
    "    swap_entry['cur_state'] = 'not_created'\n",
    "    swap_entry['cur_sold_count'] = 0\n",
    "    swap_entry['closed_stamp'] = -1\n",
    "    swap_entry['closed_iso_date'] = ''\n",
    "    swap_entry['closed_row_id'] = -1\n",
    "    swap_entry['closed_hash'] = ''\n",
    "\n",
    "\n",
    "for nft_log_entry in nft_state_log:\n",
    "    entry_method = nft_log_entry['method']\n",
    "    assert entry_method in ['apply_transfer', 'apply_mint'], entry_method\n",
    "\n",
    "    entry_volume = nft_log_entry['volume']\n",
    "    ah_log_entry = tr_hash_to_ah_log_entry.get(nft_log_entry['hash'])\n",
    "    money_log_entry = tr_hash_to_money_log_entry.get(nft_log_entry['hash'])\n",
    "\n",
    "    if entry_method == 'apply_mint':\n",
    "        assert entry_volume == 0\n",
    "        assert ah_log_entry['method'] == 'apply_mint'\n",
    "\n",
    "        entry_token_id = nft_log_entry['token_id']\n",
    "        entry_count = nft_log_entry['count']\n",
    "        assert ah_log_entry['count'] == nft_log_entry['count']\n",
    "        assert ah_log_entry['creator'] == nft_log_entry['creator']\n",
    "        token_entry = tokens_db[str(entry_token_id)]\n",
    "\n",
    "        total_stats['cur_author_count'] += entry_count\n",
    "        token_entry['cur_author_count'] += entry_count\n",
    "        continue\n",
    "\n",
    "    for entry_tx in nft_log_entry['txs']:\n",
    "        tx_from = entry_tx['from']\n",
    "        tx_to = entry_tx['to']\n",
    "        tx_token_id = int(entry_tx['token_id'])\n",
    "        tx_count = int(entry_tx['count'])\n",
    "        \n",
    "        if tx_count == 0:\n",
    "            continue\n",
    "\n",
    "        token_entry = tokens_db[str(tx_token_id)]\n",
    "        token_creator = token_entry['creator']\n",
    "        \n",
    "        tr_class = None\n",
    "        \n",
    "        assert tx_from != tx_to\n",
    "\n",
    "        if tx_from == token_creator:\n",
    "            assert entry_volume == 0\n",
    "\n",
    "            if tx_to == art_house_addr:\n",
    "                if not ah_log_entry:\n",
    "                    tr_class = ('author', 'trash')\n",
    "                    assert entry_volume == 0\n",
    "\n",
    "                else:\n",
    "                    tr_class = ('author', 'swap')\n",
    "                    assert ah_log_entry['method'] == 'apply_swap'\n",
    "                    assert entry_volume == 0\n",
    "                    swap_entry = swaps_by_row_id[ah_log_entry['row_id']]\n",
    "                    assert swap_entry['cur_state'] == 'not_created'\n",
    "                    swap_entry['cur_state'] = 'active'\n",
    "\n",
    "            elif tx_to == trash_addr:\n",
    "                tr_class = ('author', 'trash')\n",
    "                assert ah_log_entry is None\n",
    "                assert entry_volume == 0\n",
    "\n",
    "            else:\n",
    "                tr_class = ('author', 'other')\n",
    "                assert ah_log_entry is None\n",
    "                assert entry_volume == 0\n",
    "\n",
    "        elif tx_from == art_house_addr:\n",
    "            if tx_to == token_creator:\n",
    "                tr_class = ('swap', 'author')\n",
    "                assert ah_log_entry['method'] == 'apply_cancel_swap'\n",
    "                assert entry_volume == 0\n",
    "                swap_entry = swaps_db[str(ah_log_entry['swap_id'])]\n",
    "                assert swap_entry['cur_state'] == 'active'\n",
    "                swap_entry['cur_state'] = 'closed'\n",
    "\n",
    "            elif tx_to == trash_addr:\n",
    "                assert False\n",
    "\n",
    "            else:\n",
    "                tr_class = ('swap', 'other')\n",
    "                assert ah_log_entry['method'] == 'apply_collect'\n",
    "                assert entry_volume >= 0\n",
    "                swap_entry = swaps_db[str(ah_log_entry['swap_id'])]\n",
    "                assert swap_entry['cur_state'] == 'active'\n",
    "                assert ah_log_entry['count'] == tx_count\n",
    "\n",
    "                assert len(nft_log_entry['txs']) == 1\n",
    "\n",
    "                if swap_entry['price'] == 0:\n",
    "                    assert money_log_entry is None\n",
    "\n",
    "                else:\n",
    "                    assert money_log_entry\n",
    "                    assert money_log_entry['token_id'] == swap_entry['token_id']\n",
    "                    assert money_log_entry['token_count'] == tx_count\n",
    "                    assert abs(round(money_log_entry['price'] * 1e6) - swap_entry['price'] * tx_count) <= 1, (\n",
    "                        round(money_log_entry['price'] * 1e6),\n",
    "                        swap_entry['price'],\n",
    "                    )\n",
    "                    assert money_log_entry['beneficiary'] == token_creator\n",
    "\n",
    "                purchases_log.append({\n",
    "                    'token_id': swap_entry['token_id'],\n",
    "                    'swap_id': swap_entry['swap_id'],\n",
    "                    'price': swap_entry['price'],\n",
    "                    'token_count': tx_count,\n",
    "                    'token_creator': token_creator,\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            assert entry_volume == 0\n",
    "\n",
    "            if tx_to == token_creator:\n",
    "                tr_class = ('other', 'author')\n",
    "                assert ah_log_entry is None\n",
    "\n",
    "            elif tx_to == art_house_addr:\n",
    "                # print(ah_log_entry)\n",
    "                # print(nft_log_entry)\n",
    "                # print(token_entry)\n",
    "                # print()\n",
    "                tr_class = ('other', 'swap')\n",
    "                assert ah_log_entry['method'] == 'apply_swap'\n",
    "\n",
    "            else:\n",
    "                tr_class = ('other', 'other')\n",
    "                assert ah_log_entry is None\n",
    "\n",
    "        sender_class, receiver_class = tr_class\n",
    "        for stats_entry in [token_entry, total_stats]:\n",
    "            token_entry[f'cur_{sender_class}_count'] -= tx_count\n",
    "            token_entry[f'cur_{receiver_class}_count'] += tx_count\n",
    "            token_entry[f'cur_{sender_class}2{receiver_class}_count'] += tx_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write datasets files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written 582 bytes 2 entries\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def make_dataset(db, fpath):\n",
    "    expanded_db = {}\n",
    "    for row_id, row in db.items():\n",
    "        new_row = {}\n",
    "        for field, val in row.items():\n",
    "            if isinstance(val, FieldsGroup):\n",
    "                for exp_field, exp_val in val.expand_fields(field).items():\n",
    "                    new_row[exp_field] = exp_val\n",
    "            else:\n",
    "                assert type(val) in [int, float, str]\n",
    "                new_row[field] = val\n",
    "        expanded_db[row_id] = new_row\n",
    "\n",
    "    rows = list(expanded_db.values())\n",
    "    cols_order = list(rows[0].keys())\n",
    "    for row in rows:\n",
    "        row_keys_order = list(row.keys())\n",
    "        assert row_keys_order == cols_order, (row_keys_order, cols_order)\n",
    "\n",
    "    with fpath.with_suffix('.csv').open('w', encoding='utf-8', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(\n",
    "            csv_file,\n",
    "            delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL,\n",
    "        )\n",
    "        csv_writer.writerow([col for col in cols_order])\n",
    "        for row in rows:\n",
    "            csv_writer.writerow([row[col] for col in cols_order])\n",
    "\n",
    "    src.utils.write_json(expanded_db, fpath.with_suffix('.json'))\n",
    "\n",
    "\n",
    "\n",
    "db = {\n",
    "    'x': {\n",
    "        'a': 1,\n",
    "        'b': 'x and y',\n",
    "        'event': TrEvent(42212375),\n",
    "        'float_set': FloatSet([0, 1, 2, 3]),\n",
    "    },\n",
    "    'y': {\n",
    "        'a': 2,\n",
    "        'b': 'y',\n",
    "        'event': TrEvent(42212375),\n",
    "        'float_set': FloatSet([0, 2, 4, 8]),\n",
    "    }\n",
    "}\n",
    "\n",
    "make_dataset(db, Path('../dataset/test'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
